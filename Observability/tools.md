The real-world observability stack most cloud-native teams use:

Prometheus â†’ Metrics

Grafana â†’ Visualization

ELK â†’ Logs

Weâ€™ll explain this in a production mindset.

ğŸ§  Big Picture First

Modern production stack typically looks like:

Applications
â†“
Metrics â†’ Prometheus
Logs â†’ ELK / Loki
â†“
Grafana dashboards
â†“
Alertmanager / Slack / PagerDuty

1ï¸âƒ£ Prometheus (Metrics Engine)
Simple Definition

Prometheus is a time-series database that collects and stores metrics.

It answers:

Is the system healthy?

How fast is it?

How much load is it handling?

How Prometheus Works

Applications expose metrics at /metrics

Prometheus scrapes them periodically

Stores time-series data

You query using PromQL

Alertmanager sends alerts

Example Metrics
http_requests_total
http_request_duration_seconds
container_cpu_usage_seconds_total

In Kubernetes

Prometheus monitors:

Nodes

Pods

Containers

API server

kubelet

etcd

Custom app metrics

Usually deployed via:

kube-prometheus-stack (Helm chart)

Why Prometheus Is Powerful

Pull-based model (safer than push)

Native Kubernetes integration

Label-based querying

Works well with autoscaling

2ï¸âƒ£ Grafana (Visualization Layer)
Simple Definition

Grafana visualizes data.

It connects to:

Prometheus

Elasticsearch

Loki

CloudWatch

Many others

What Grafana Does

Dashboards

Alert visualization

Drill-down exploration

Multi-data source integration

Example dashboard:

CPU usage

Memory usage

Request latency

Error rate

All in one screen.

3ï¸âƒ£ ELK Stack (Logging)

ELK =

Elasticsearch (storage + search)

Logstash (log processor)

Kibana (UI)

How ELK Works

App logs
â†“
Fluentd / Filebeat
â†“
Logstash
â†“
Elasticsearch
â†“
Kibana UI

What ELK Solves

Centralized logs

Full-text search

Root cause debugging

Audit compliance

4ï¸âƒ£ Real Production Architecture

In Kubernetes:

Pods
â†“
stdout logs
â†“
FluentBit DaemonSet
â†“
Elasticsearch
â†“
Kibana

Metrics:

Pods
â†“
Prometheus scrapes
â†“
Stores time-series
â†“
Grafana dashboards

5ï¸âƒ£ Real Incident Scenario

Problem:
API latency spike.

Step 1:
Grafana dashboard â†’ latency increased.

Step 2:
Prometheus â†’ error rate rising.

Step 3:
Kibana â†’ DB connection timeout logs.

Root cause identified.

Without observability:
Youâ€™d be guessing.

6ï¸âƒ£ Why Not Just Logs?

Logs are detailed but:

High volume

Hard to aggregate

Slower to query

Metrics:

Lightweight

Fast

Better for alerting

Logs:

Better for root cause

Traces:

Best for distributed debugging

All three are needed.

7ï¸âƒ£ Production Best Practices
Metrics

Instrument app (Prometheus client)

Track golden signals

Use labels wisely (avoid cardinality explosion)

Logging

Structured JSON logs

Correlation ID per request

No secrets in logs

Set retention policy

Alerts

Alert on:

Error rate

Latency SLO violation

Saturation

Pod crash loops

Avoid alerting on:

Every pod restart

Every CPU spike

8ï¸âƒ£ Common Production Mistakes

No metric instrumentation in app

Too many logs (cost explosion)

High-cardinality metrics

No retention strategy

No runbooks

Alert fatigue

9ï¸âƒ£ Prometheus vs ELK
Feature	Prometheus	ELK
Purpose	Metrics	Logs
Storage	Time-series	Document-based
Alerting	Built-in	Not native
Query	PromQL	Lucene DSL
Volume	Low-medium	High

They complement each other.

ğŸ”Ÿ Modern Alternative Stack

Many teams now use:

Prometheus + Grafana + Loki + Tempo

Instead of full ELK.

Loki is lighter than Elasticsearch.

ğŸ”¥ Real-World Enterprise Setup

EKS Cluster
â†“
kube-prometheus-stack
â†“
Prometheus + Alertmanager
â†“
Grafana
â†“
FluentBit
â†“
Elasticsearch (or Loki)
â†“
Slack / PagerDuty alerts

Everything centralized.

ğŸ¯ Final Understanding

Prometheus = numerical health
Grafana = visualization
ELK = event details

Together:
They give production confidence.